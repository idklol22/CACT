{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64982882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "Loading data...\n",
      "train: 4500 val: 500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe8e33c388d4d12846eb4d64d74a853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sanjam\\miniconda3\\envs\\env_isaaclab\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sanjam\\.cache\\huggingface\\hub\\models--albert-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c582f488bff43ea9b1c9fe6db93b8d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ea2716267c44048100e9585c826e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08a1883ce124dcdb7f32e4e69f75356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda1a9ffe6c844b2afe54bf2030d8157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1/2: 100%|██████████| 141/141 [1:14:52<00:00, 31.86s/it, L=0.685, CE=0.68, C=0.0109]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2/2: 100%|██████████| 141/141 [1:16:21<00:00, 32.49s/it, L=0.661, CE=0.658, C=0.00637]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.768\n",
      "saved: ckpts/cact_albert.pt\n"
     ]
    }
   ],
   "source": [
    "import os, json, random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------- CONFIG -------------------\n",
    "TRAIN_PATH = \"data/imdb_triplets_train.jsonl\"   # set this\n",
    "VAL_PATH   = \"data/imdb_triplets_val.jsonl\"     # set this\n",
    "\n",
    "MODEL_NAME = \"albert-base-v2\"\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "LR = 2e-5\n",
    "MAX_LEN = 256\n",
    "\n",
    "TAU = 0.07\n",
    "LAMBDA_CONT = 0.5\n",
    "CE_ON_ALL_VIEWS = True   # CE on anchor+para+style\n",
    "SEED = 42\n",
    "# ---------------------------------------------\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def read_jsonl(path: str) -> List[Dict]:\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "class CACTTripletDataset(Dataset):\n",
    "    def __init__(self, rows: List[Dict]):\n",
    "        self.rows = rows\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        r = self.rows[idx]\n",
    "        return {\n",
    "            \"anchor\": r[\"anchor\"],\n",
    "            \"para\": r[\"positive_para\"],\n",
    "            \"style\": r[\"positive_style\"],\n",
    "            \"label\": int(r[\"label\"]),\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class Batch:\n",
    "    a_ids: torch.Tensor\n",
    "    a_attn: torch.Tensor\n",
    "    p_ids: torch.Tensor\n",
    "    p_attn: torch.Tensor\n",
    "    s_ids: torch.Tensor\n",
    "    s_attn: torch.Tensor\n",
    "    y: torch.Tensor\n",
    "\n",
    "def collate(tok, items: List[Dict]) -> Batch:\n",
    "    anchors = [x[\"anchor\"] for x in items]\n",
    "    paras   = [x[\"para\"] for x in items]\n",
    "    styles  = [x[\"style\"] for x in items]\n",
    "    y = torch.tensor([x[\"label\"] for x in items], dtype=torch.long)\n",
    "\n",
    "    def enc(texts):\n",
    "        out = tok(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return out[\"input_ids\"], out[\"attention_mask\"]\n",
    "\n",
    "    a_ids, a_attn = enc(anchors)\n",
    "    p_ids, p_attn = enc(paras)\n",
    "    s_ids, s_attn = enc(styles)\n",
    "\n",
    "    return Batch(a_ids, a_attn, p_ids, p_attn, s_ids, s_attn, y)\n",
    "\n",
    "class CACTModel(nn.Module):\n",
    "    def __init__(self, backbone: str):\n",
    "        super().__init__()\n",
    "        self.enc = AutoModel.from_pretrained(backbone)\n",
    "        hid = self.enc.config.hidden_size\n",
    "        self.cls = nn.Linear(hid, 2)\n",
    "\n",
    "    def embed(self, input_ids, attn_mask):\n",
    "        out = self.enc(input_ids=input_ids, attention_mask=attn_mask)\n",
    "        h = out.last_hidden_state[:, 0]          # [CLS]\n",
    "        h = F.normalize(h, dim=-1)               # important for cosine similarity\n",
    "        return h\n",
    "\n",
    "    def logits(self, h):\n",
    "        return self.cls(h)\n",
    "\n",
    "def two_pos_infonce(h_a, h_p, h_s, tau: float):\n",
    "    \"\"\"\n",
    "    positives: (a,p) and (a,s)\n",
    "    negatives: other anchors in batch\n",
    "    \"\"\"\n",
    "    B = h_a.size(0)\n",
    "\n",
    "    sim_aa = (h_a @ h_a.t()) / tau              # [B,B]\n",
    "    sim_ap = (h_a * h_p).sum(dim=-1, keepdim=True) / tau  # [B,1]\n",
    "    sim_as = (h_a * h_s).sum(dim=-1, keepdim=True) / tau  # [B,1]\n",
    "\n",
    "    eye = torch.eye(B, device=h_a.device).bool()\n",
    "    sim_aa = sim_aa.masked_fill(eye, -1e9)      # remove self-neg\n",
    "\n",
    "    num = torch.exp(sim_ap) + torch.exp(sim_as)                 # [B,1]\n",
    "    den = num + torch.exp(sim_aa).sum(dim=1, keepdim=True)       # [B,1]\n",
    "    return (-torch.log(num / (den + 1e-12))).mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dl, tok):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for batch in dl:\n",
    "        h = model.embed(batch.a_ids.to(DEVICE), batch.a_attn.to(DEVICE))\n",
    "        logits = model.logits(h)\n",
    "        pred = logits.argmax(dim=-1).cpu()\n",
    "        correct += (pred == batch.y).sum().item()\n",
    "        total += batch.y.size(0)\n",
    "    model.train()\n",
    "    return correct / max(total, 1)\n",
    "\n",
    "def main():\n",
    "    random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    print(\"DEVICE:\", DEVICE)\n",
    "    print(\"Loading data...\")\n",
    "    train_rows = read_jsonl(TRAIN_PATH)\n",
    "    val_rows = read_jsonl(VAL_PATH)\n",
    "    print(\"train:\", len(train_rows), \"val:\", len(val_rows))\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    train_ds = CACTTripletDataset(train_rows)\n",
    "    val_ds = CACTTripletDataset(val_rows)\n",
    "\n",
    "    train_dl = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda x: collate(tok, x),\n",
    "        num_workers=0,\n",
    "    )\n",
    "    val_dl = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x: collate(tok, x),\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    model = CACTModel(MODEL_NAME).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    total_steps = EPOCHS * len(train_dl)\n",
    "    sched = get_linear_schedule_with_warmup(\n",
    "        opt,\n",
    "        num_warmup_steps=int(0.06 * total_steps),\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "\n",
    "    for ep in range(EPOCHS):\n",
    "        pbar = tqdm(train_dl, desc=f\"epoch {ep+1}/{EPOCHS}\")\n",
    "        for batch in pbar:\n",
    "            a_ids, a_attn = batch.a_ids.to(DEVICE), batch.a_attn.to(DEVICE)\n",
    "            p_ids, p_attn = batch.p_ids.to(DEVICE), batch.p_attn.to(DEVICE)\n",
    "            s_ids, s_attn = batch.s_ids.to(DEVICE), batch.s_attn.to(DEVICE)\n",
    "            y = batch.y.to(DEVICE)\n",
    "\n",
    "            h_a = model.embed(a_ids, a_attn)\n",
    "            h_p = model.embed(p_ids, p_attn)\n",
    "            h_s = model.embed(s_ids, s_attn)\n",
    "\n",
    "            # CE (label preservation)\n",
    "            L_ce = ce(model.logits(h_a), y)\n",
    "            if CE_ON_ALL_VIEWS:\n",
    "                L_ce = (L_ce + ce(model.logits(h_p), y) + ce(model.logits(h_s), y)) / 3.0\n",
    "\n",
    "            # Contrastive invariance (style + paraphrase)\n",
    "            L_cont = two_pos_infonce(h_a, h_p, h_s, tau=TAU)\n",
    "\n",
    "            loss = L_ce + LAMBDA_CONT * L_cont\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            sched.step()\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"L\": float(loss.detach().cpu()),\n",
    "                \"CE\": float(L_ce.detach().cpu()),\n",
    "                \"C\": float(L_cont.detach().cpu())\n",
    "            })\n",
    "\n",
    "        acc = evaluate(model, val_dl, tok)\n",
    "        print(\"val_acc:\", acc)\n",
    "\n",
    "    os.makedirs(\"ckpts\", exist_ok=True)\n",
    "    torch.save(model.state_dict(), \"ckpts/cact_albert.pt\")\n",
    "    print(\"saved: ckpts/cact_albert.pt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f7504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_isaaclab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
