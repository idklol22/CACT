{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3143ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc06229a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "Loading models...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c9debc3150441bbc362b137a182ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sanjam\\miniconda3\\envs\\env_isaaclab\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sanjam\\.cache\\huggingface\\hub\\models--Vamsi--T5_Paraphrase_Paws. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9a7ab177ce4626add32d8fbb8d18a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23fcde81fa4b40e39865e1175449ead3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653dc97666554a45a051f6b4dd6b9ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62fa6a68a6b445cba64d67b7b424b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a60c6e1805b43e2ab42bc0c21e0a567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sanjam\\miniconda3\\envs\\env_isaaclab\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sanjam\\.cache\\huggingface\\hub\\models--google--flan-t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196f051d95184de19266bff0ff1ae307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b9adc2babf4c3698de1f3dc31c7029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d542ec5b8da94ebc93ab85b8cd801f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff85e7c10884bbebfc3f36f39b03d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276ebdf0c5474e9eb7feb39050a41f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3fc913fa8c4f8fb75a45c4bf45d5bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDb...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350696e5ded34decac8bade0ce8aec4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a6e7572d044730b7b59eae330ed2da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [25:03<00:00,  9.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: data/imdb_triplets.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os, re, json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "PARA_MODEL  = \"Vamsi/T5_Paraphrase_Paws\"     # paraphrase-tuned [web:250]\n",
    "STYLE_MODEL = \"google/flan-t5-base\"          # better at instruction-following [web:268]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MAX_SAMPLES = 5000          # keep 5k for testing first\n",
    "OUT_PATH = \"data/imdb_triplets.jsonl\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16 if DEVICE.startswith(\"cuda\") else torch.float32\n",
    "\n",
    "MAX_IN_TOKENS = 256         # keep inputs shorter than full IMDb reviews\n",
    "MAX_NEW_TOKENS = 128        # cap generation length\n",
    "\n",
    "SEED = 42\n",
    "# ----------------------------------------\n",
    "\n",
    "def clean_html(text: str) -> str:\n",
    "    # remove common IMDb HTML breaks\n",
    "    text = text.replace(\"<br />\", \" \").replace(\"<br/>\", \" \").replace(\"<br>\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def shorten(text: str, max_chars: int = 900) -> str:\n",
    "    # cheap, stable shortening for IMDb long reviews\n",
    "    return text[:max_chars]\n",
    "\n",
    "def is_bad(gen: str) -> bool:\n",
    "    g = gen.strip().lower()\n",
    "    if g in {\"true\", \"false\"}:\n",
    "        return True\n",
    "    if len(g) < 20:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_batch(model, tok, prompts):\n",
    "    enc = tok(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_IN_TOKENS,\n",
    "    )\n",
    "    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "\n",
    "    out_ids = model.generate(\n",
    "        **enc,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,   # prefer this over max_length [web:268][web:226]\n",
    "        num_beams=4,\n",
    "        do_sample=False,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    return [tok.decode(x, skip_special_tokens=True).strip() for x in out_ids]\n",
    "\n",
    "def maybe_resume_count(path):\n",
    "    if not os.path.exists(path):\n",
    "        return 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return sum(1 for _ in f)\n",
    "\n",
    "def main():\n",
    "    os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)\n",
    "    print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "    print(\"Loading models...\")\n",
    "    para_tok = AutoTokenizer.from_pretrained(PARA_MODEL)\n",
    "    para_mod = AutoModelForSeq2SeqLM.from_pretrained(PARA_MODEL, torch_dtype=DTYPE).to(DEVICE).eval()\n",
    "\n",
    "    style_tok = AutoTokenizer.from_pretrained(STYLE_MODEL)\n",
    "    style_mod = AutoModelForSeq2SeqLM.from_pretrained(STYLE_MODEL, torch_dtype=DTYPE).to(DEVICE).eval()\n",
    "\n",
    "    print(\"Loading IMDb...\")\n",
    "    ds = load_dataset(\"imdb\", split=\"train\")\n",
    "\n",
    "    # balanced subset\n",
    "    pos = ds.filter(lambda x: x[\"label\"] == 1).shuffle(seed=SEED)\n",
    "    neg = ds.filter(lambda x: x[\"label\"] == 0).shuffle(seed=SEED)\n",
    "    pos = pos.select(range(MAX_SAMPLES // 2))\n",
    "    neg = neg.select(range(MAX_SAMPLES // 2))\n",
    "\n",
    "    data = concatenate_datasets([pos, neg]).shuffle(seed=SEED)  # [web:273]\n",
    "    data = data.remove_columns([c for c in data.column_names if c not in [\"text\", \"label\"]])\n",
    "\n",
    "    # resume support\n",
    "    already = maybe_resume_count(OUT_PATH)\n",
    "    if already > 0:\n",
    "        print(f\"Resuming: {already} lines already in {OUT_PATH}\")\n",
    "    start_idx = already\n",
    "    if start_idx >= len(data):\n",
    "        print(\"Nothing to do; file already complete.\")\n",
    "        return\n",
    "\n",
    "    # iterate in slices (fast + deterministic)\n",
    "    with open(OUT_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "        for start in tqdm(range(start_idx, len(data), BATCH_SIZE)):\n",
    "            batch = data[start : start + BATCH_SIZE]\n",
    "            anchors_raw = batch[\"text\"]\n",
    "            labels = batch[\"label\"]\n",
    "\n",
    "            anchors = [shorten(clean_html(t)) for t in anchors_raw]\n",
    "\n",
    "            # prompts\n",
    "            para_prompts = [f\"paraphrase: {a} </s>\" for a in anchors]  # matches model card style [web:252]\n",
    "            style_prompts = [f\"Rewrite as a short casual tweet, keep sentiment the same: {a}\" for a in anchors]\n",
    "\n",
    "            paras = generate_batch(para_mod, para_tok, para_prompts)\n",
    "            styles = generate_batch(style_mod, style_tok, style_prompts)\n",
    "\n",
    "            for a, p, s, y in zip(anchors, paras, styles, labels):\n",
    "                # fallback if generation is broken\n",
    "                if is_bad(p): p = a\n",
    "                if is_bad(s): s = a\n",
    "\n",
    "                rec = {\n",
    "                    \"anchor\": a,\n",
    "                    \"positive_para\": p,\n",
    "                    \"positive_style\": s,\n",
    "                    \"label\": int(y),\n",
    "                }\n",
    "                f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(\"Done:\", OUT_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeea1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\sanjam\\miniconda3\\envs\\env_isaaclab\\lib\\site-packages (0.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_isaaclab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
