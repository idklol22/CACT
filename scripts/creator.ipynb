{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3143ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc06229a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "Loading models...\n",
      "Loading IMDb...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 35/157 [05:54<20:48, 10.23s/it]"
     ]
    }
   ],
   "source": [
    "import os, re, json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "PARA_MODEL  = \"Vamsi/T5_Paraphrase_Paws\"     # paraphrase-tuned [web:250]\n",
    "STYLE_MODEL = \"google/flan-t5-base\"          # better at instruction-following [web:268]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MAX_SAMPLES = 5000          # keep 5k for testing first\n",
    "OUT_PATH = \"data/imdb_triplets.jsonl\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16 if DEVICE.startswith(\"cuda\") else torch.float32\n",
    "\n",
    "MAX_IN_TOKENS = 256         # keep inputs shorter than full IMDb reviews\n",
    "MAX_NEW_TOKENS = 128        # cap generation length\n",
    "\n",
    "SEED = 42\n",
    "# ----------------------------------------\n",
    "\n",
    "def clean_html(text: str) -> str:\n",
    "    # remove common IMDb HTML breaks\n",
    "    text = text.replace(\"<br />\", \" \").replace(\"<br/>\", \" \").replace(\"<br>\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def shorten(text: str, max_chars: int = 900) -> str:\n",
    "    # cheap, stable shortening for IMDb long reviews\n",
    "    return text[:max_chars]\n",
    "\n",
    "def is_bad(gen: str) -> bool:\n",
    "    g = gen.strip().lower()\n",
    "    if g in {\"true\", \"false\"}:\n",
    "        return True\n",
    "    if len(g) < 20:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_batch(model, tok, prompts):\n",
    "    enc = tok(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_IN_TOKENS,\n",
    "    )\n",
    "    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "\n",
    "    out_ids = model.generate(\n",
    "        **enc,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,   # prefer this over max_length [web:268][web:226]\n",
    "        num_beams=4,\n",
    "        do_sample=False,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    return [tok.decode(x, skip_special_tokens=True).strip() for x in out_ids]\n",
    "\n",
    "def maybe_resume_count(path):\n",
    "    if not os.path.exists(path):\n",
    "        return 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return sum(1 for _ in f)\n",
    "\n",
    "def main():\n",
    "    os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)\n",
    "    print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "    print(\"Loading models...\")\n",
    "    para_tok = AutoTokenizer.from_pretrained(PARA_MODEL)\n",
    "    para_mod = AutoModelForSeq2SeqLM.from_pretrained(PARA_MODEL, torch_dtype=DTYPE).to(DEVICE).eval()\n",
    "\n",
    "    style_tok = AutoTokenizer.from_pretrained(STYLE_MODEL)\n",
    "    style_mod = AutoModelForSeq2SeqLM.from_pretrained(STYLE_MODEL, torch_dtype=DTYPE).to(DEVICE).eval()\n",
    "\n",
    "    print(\"Loading IMDb...\")\n",
    "    ds = load_dataset(\"imdb\", split=\"train\")\n",
    "\n",
    "    # balanced subset\n",
    "    pos = ds.filter(lambda x: x[\"label\"] == 1).shuffle(seed=SEED)\n",
    "    neg = ds.filter(lambda x: x[\"label\"] == 0).shuffle(seed=SEED)\n",
    "    pos = pos.select(range(MAX_SAMPLES // 2))\n",
    "    neg = neg.select(range(MAX_SAMPLES // 2))\n",
    "\n",
    "    data = concatenate_datasets([pos, neg]).shuffle(seed=SEED)  # [web:273]\n",
    "    data = data.remove_columns([c for c in data.column_names if c not in [\"text\", \"label\"]])\n",
    "\n",
    "    # resume support\n",
    "    already = maybe_resume_count(OUT_PATH)\n",
    "    if already > 0:\n",
    "        print(f\"Resuming: {already} lines already in {OUT_PATH}\")\n",
    "    start_idx = already\n",
    "    if start_idx >= len(data):\n",
    "        print(\"Nothing to do; file already complete.\")\n",
    "        return\n",
    "\n",
    "    # iterate in slices (fast + deterministic)\n",
    "    with open(OUT_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "        for start in tqdm(range(start_idx, len(data), BATCH_SIZE)):\n",
    "            batch = data[start : start + BATCH_SIZE]\n",
    "            anchors_raw = batch[\"text\"]\n",
    "            labels = batch[\"label\"]\n",
    "\n",
    "            anchors = [shorten(clean_html(t)) for t in anchors_raw]\n",
    "\n",
    "            # prompts\n",
    "            para_prompts = [f\"paraphrase: {a} </s>\" for a in anchors]  # matches model card style [web:252]\n",
    "            style_prompts = [f\"Rewrite as a short casual tweet, keep sentiment the same: {a}\" for a in anchors]\n",
    "\n",
    "            paras = generate_batch(para_mod, para_tok, para_prompts)\n",
    "            styles = generate_batch(style_mod, style_tok, style_prompts)\n",
    "\n",
    "            for a, p, s, y in zip(anchors, paras, styles, labels):\n",
    "                # fallback if generation is broken\n",
    "                if is_bad(p): p = a\n",
    "                if is_bad(s): s = a\n",
    "\n",
    "                rec = {\n",
    "                    \"anchor\": a,\n",
    "                    \"positive_para\": p,\n",
    "                    \"positive_style\": s,\n",
    "                    \"label\": int(y),\n",
    "                }\n",
    "                f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(\"Done:\", OUT_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeea1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\sanjam\\miniconda3\\envs\\env_isaaclab\\lib\\site-packages (0.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_isaaclab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
